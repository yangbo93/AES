
Notably, all baseline attack methods use official codes, and we appreciate the contributions of these authors to the research community of adversarial attacks.

For gradient stabilization attacks, the Code refer to: [Penelizing Gradient Norm Attack](https://github.com/Trustworthy-AI-Group/PGN) and [Spectrum Simulation Attack](https://github.com/yuyang-long/SSA).

For input transformation attacks, Code refer to: [Spectrum Simulation Attack](https://github.com/yuyang-long/SSA).

For Feature destruction attacks, Code refer to: [Neuron Attribution-Based Attacks](https://github.com/jpzhang1810/NAA).

For the formation of adversarial example soups, it is only necessary to average the images of multi-batche adversarial examples.

